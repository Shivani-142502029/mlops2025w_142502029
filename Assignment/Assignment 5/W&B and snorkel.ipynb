{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2bf347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Prepared 51362 dev tokens and 203621 train tokens.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from snorkel.labeling import labeling_function, LFAnalysis, PandasLFApplier\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "\n",
    "# Snorkel Label Mapping\n",
    "# 0: ABSTAIN (No label)\n",
    "# 1-4: The four entity types (simplified)\n",
    "ABSTAIN = 0\n",
    "ORG = 1\n",
    "MISC = 2\n",
    "PER = 3\n",
    "LOC = 4\n",
    "\n",
    "# Data Loading (Using the stable Parquet revision)\n",
    "dataset_name = \"conll2003\"\n",
    "parquet_revision = \"refs/convert/parquet\"\n",
    "print(\"Loading dataset...\")\n",
    "raw_datasets = load_dataset(dataset_name, revision=parquet_revision)\n",
    "TAG_NAMES = raw_datasets['train'].features['ner_tags'].feature.names\n",
    "print(f\"Dataset loaded successfully: {raw_datasets}\")\n",
    "\n",
    "# Helper Function for Snorkel Data Format\n",
    "def prepare_snorkel_data(data_split):\n",
    "    records = []\n",
    "    for example in data_split:\n",
    "        tokens = example['tokens']\n",
    "        ner_tags = example['ner_tags']\n",
    "        for token, tag_id in zip(tokens, ner_tags):\n",
    "            tag_name = TAG_NAMES[tag_id]\n",
    "            target_label = ABSTAIN \n",
    "            if 'PER' in tag_name:\n",
    "                target_label = PER\n",
    "            elif 'LOC' in tag_name:\n",
    "                target_label = LOC\n",
    "            elif 'ORG' in tag_name:\n",
    "                target_label = ORG\n",
    "            elif 'MISC' in tag_name:\n",
    "                target_label = MISC\n",
    "            records.append({'token': token, 'target': target_label, 'token_text': token})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Prepare the dataframes\n",
    "df_dev = prepare_snorkel_data(raw_datasets['validation'])\n",
    "df_train = prepare_snorkel_data(raw_datasets['train'])\n",
    "print(f\"Prepared {len(df_dev)} dev tokens and {len(df_train)} train tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c31f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\2021i\\Documents\\10_iit palakkad\\MLOPS\\Assignment 5\\wandb\\run-20251014_003854-wdc3d2ub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner/runs/wdc3d2ub' target=\"_blank\">rural-dew-26</a></strong> to <a href='https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner/runs/wdc3d2ub' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner/runs/wdc3d2ub</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B project initialized for Q1, Q2, and Q3 metrics.\n",
      "\n",
      "[Q1] Dataset Sample Counts: {'train_samples': 14041, 'validation_samples': 3250, 'test_samples': 3453, 'total_samples': 20744}\n",
      "[Q1] Entity Distribution:\n",
      "  Entity Type  Count\n",
      "2         PER  17050\n",
      "0         ORG  14613\n",
      "3         LOC  12316\n",
      "1        MISC   6779\n",
      "Logged dataset sample counts to W&B summary.\n",
      "Logged entity distribution table to W&B run.\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Weights & Biases Project\n",
    "wandb.init(project=\"Q1-weak-supervision-ner\")\n",
    "print(\"W&B project initialized for Q1, Q2, and Q3 metrics.\")\n",
    "\n",
    "# 2. Calculate and Log Dataset Statistics\n",
    "\n",
    "# Calculate Sample Counts\n",
    "\n",
    "dataset_stats = {\n",
    "    'train_samples': len(raw_datasets['train']),\n",
    "    'validation_samples': len(raw_datasets['validation']),\n",
    "    'test_samples': len(raw_datasets['test']),\n",
    "}\n",
    "dataset_stats['total_samples'] = sum(dataset_stats.values())\n",
    "print(\"\\n[Q1] Dataset Sample Counts:\", dataset_stats)\n",
    "\n",
    "# Calculate Entity Distribution\n",
    "entity_counts = {}\n",
    "for split in raw_datasets:\n",
    "    for tags in raw_datasets[split]['ner_tags']:\n",
    "        for tag_id in tags:\n",
    "            tag_name = TAG_NAMES[tag_id]\n",
    "            if tag_name.startswith(('B-', 'I-')):\n",
    "                entity_type = tag_name.split('-')[-1]\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "\n",
    "entity_df = pd.DataFrame(\n",
    "    list(entity_counts.items()),\n",
    "    columns=['Entity Type', 'Count']\n",
    ").sort_values(by='Count', ascending=False)\n",
    "print(\"[Q1] Entity Distribution:\")\n",
    "print(entity_df)\n",
    "\n",
    "# Log Statistics to W&B\n",
    "wandb.run.summary.update(dataset_stats)\n",
    "print(\"Logged dataset sample counts to W&B summary.\")\n",
    "entity_table = wandb.Table(dataframe=entity_df)\n",
    "wandb.log({\"Q1/Entity Distribution\": entity_table})\n",
    "print(\"Logged entity distribution table to W&B run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22994dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q2a] LF for Years/MISC defined.\n",
      "[Q2b] LF for Organization Suffixes/ORG defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 51362/51362 [00:01<00:00, 40612.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q2] Calculating and Logging LF Metrics (Coverage & Accuracy)\n",
      "  lf_year_as_misc: Coverage=0.0073, Accuracy=0.0187 (Logged to W&B)\n",
      "  lf_org_suffix: Coverage=0.0004, Accuracy=0.8947 (Logged to W&B)\n",
      "[Q2] Successfully logged coverage and accuracy for both LFs.\n"
     ]
    }
   ],
   "source": [
    "# 1. Implement Labeling Functions\n",
    "\n",
    "# a. LF for Years (MISC)\n",
    "YEAR_REGEX = re.compile(r'\\b(19|20)\\d{2}\\b')\n",
    "@labeling_function()\n",
    "def lf_year_as_misc(x):\n",
    "    \"\"\"Labels four-digit numbers (1900-2099) as MISC.\"\"\"\n",
    "    if YEAR_REGEX.match(x.token_text):\n",
    "        return MISC\n",
    "    return ABSTAIN\n",
    "print(\"[Q2a] LF for Years/MISC defined.\")\n",
    "\n",
    "# b. LF for Organization Suffixes (ORG)\n",
    "ORG_SUFFIXES = ['Inc.', 'Corp.', 'Ltd.', 'Group', 'Co.', 'S.A.', 'A.G.', 'B.V.']\n",
    "@labeling_function()\n",
    "def lf_org_suffix(x):\n",
    "    \"\"\"Labels tokens ending with common organizational suffixes as ORG.\"\"\"\n",
    "    if x.token_text.strip().endswith(tuple(ORG_SUFFIXES)):\n",
    "        return ORG\n",
    "    return ABSTAIN\n",
    "print(\"[Q2b] LF for Organization Suffixes/ORG defined.\")\n",
    "\n",
    "# 2. Apply LFs and Prepare for Analysis\n",
    "lfs = [lf_year_as_misc, lf_org_suffix]\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_dev = applier.apply(df=df_dev)\n",
    "Y_dev = df_dev.target.values\n",
    "\n",
    "# 3. Calculate and Log LF Metrics (Robust Manual Fix for API issues)\n",
    "print(\"[Q2] Calculating and Logging LF Metrics (Coverage & Accuracy)\")\n",
    "\n",
    "for i, lf in enumerate(lfs):\n",
    "    # Determine the LF name safely\n",
    "    lf_name_str = lf.name if hasattr(lf, 'name') else lf.f.__name__\n",
    "    \n",
    "    L_i = L_dev[:, i]\n",
    "    \n",
    "    # Coverage: tokens where LF did NOT abstain\n",
    "    coverage = (L_i != ABSTAIN).sum() / len(L_i)\n",
    "    \n",
    "    # Accuracy: tokens where LF labeled correctly (excluding abstains)\n",
    "    covered_indices = L_i != ABSTAIN\n",
    "    Y_true_covered = Y_dev[covered_indices]\n",
    "    L_i_covered = L_i[covered_indices]\n",
    "    \n",
    "    accuracy = accuracy_score(Y_true_covered, L_i_covered) if len(Y_true_covered) > 0 else 0.0\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        f\"Q2/LF_Metrics/{lf_name_str}_Coverage\": coverage,\n",
    "        f\"Q2/LF_Metrics/{lf_name_str}_Accuracy\": accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"  {lf_name_str}: Coverage={coverage:.4f}, Accuracy={accuracy:.4f} (Logged to W&B)\")\n",
    "\n",
    "print(\"[Q2] Successfully logged coverage and accuracy for both LFs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f5d4e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 203621/203621 [00:05<00:00, 34753.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q3] Applied LFs to the training set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q3] Generated 203621 aggregated weak labels for the training set.\n",
      "[Q3] Majority Label Voter Results (Training Set)\n",
      "Coverage (Labeled Tokens): 0.0076\n",
      "Accuracy (on Covered Tokens): 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Q2/LF_Metrics/lf_org_suffix_Accuracy</td><td>▁</td></tr><tr><td>Q2/LF_Metrics/lf_org_suffix_Coverage</td><td>▁</td></tr><tr><td>Q2/LF_Metrics/lf_year_as_misc_Accuracy</td><td>▁</td></tr><tr><td>Q2/LF_Metrics/lf_year_as_misc_Coverage</td><td>▁</td></tr><tr><td>Q3/Voter_Accuracy</td><td>▁</td></tr><tr><td>Q3/Voter_Coverage</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Q2/LF_Metrics/lf_org_suffix_Accuracy</td><td>0.89474</td></tr><tr><td>Q2/LF_Metrics/lf_org_suffix_Coverage</td><td>0.00037</td></tr><tr><td>Q2/LF_Metrics/lf_year_as_misc_Accuracy</td><td>0.01867</td></tr><tr><td>Q2/LF_Metrics/lf_year_as_misc_Coverage</td><td>0.0073</td></tr><tr><td>Q3/Voter_Accuracy</td><td>0</td></tr><tr><td>Q3/Voter_Coverage</td><td>0.00762</td></tr><tr><td>test_samples</td><td>3453</td></tr><tr><td>total_samples</td><td>20744</td></tr><tr><td>train_samples</td><td>14041</td></tr><tr><td>validation_samples</td><td>3250</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-dew-26</strong> at: <a href='https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner/runs/wdc3d2ub' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner/runs/wdc3d2ub</a><br> View project at: <a href='https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q1-weak-supervision-ner</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251014_003854-wdc3d2ub\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "# 1. Apply LFs to the Training Set\n",
    "L_train = applier.apply(df=df_train)\n",
    "print(\"[Q3] Applied LFs to the training set.\")\n",
    "\n",
    "# 2. Implement Label Aggregation (Majority Label Voter)\n",
    "voter = MajorityLabelVoter(cardinality=5) \n",
    "Y_train_pred = voter.predict(L=L_train)\n",
    "print(f\"[Q3] Generated {len(Y_train_pred)} aggregated weak labels for the training set.\")\n",
    "\n",
    "# 3. Evaluate and Log Aggregation Metrics\n",
    "Y_train_true = df_train.target.values\n",
    "\n",
    "# Coverage\n",
    "coverage = (Y_train_pred != ABSTAIN).sum() / len(Y_train_pred)\n",
    "\n",
    "# Accuracy (on covered tokens)\n",
    "covered_indices = Y_train_pred != ABSTAIN\n",
    "Y_pred_covered = Y_train_pred[covered_indices]\n",
    "Y_true_covered = Y_train_true[covered_indices]\n",
    "accuracy = accuracy_score(Y_true_covered, Y_pred_covered)\n",
    "\n",
    "print(f\"[Q3] Majority Label Voter Results (Training Set)\")\n",
    "print(f\"Coverage (Labeled Tokens): {coverage:.4f}\")\n",
    "print(f\"Accuracy (on Covered Tokens): {accuracy:.4f}\")\n",
    "\n",
    "# Log Aggregation Metrics to W&B\n",
    "wandb.log({\n",
    "    \"Q3/Voter_Coverage\": coverage,\n",
    "    \"Q3/Voter_Accuracy\": accuracy\n",
    "})\n",
    "\n",
    "# 4. Finish the W&B run (must be done only once at the end)\n",
    "wandb.finish()\n",
    "print(\"W&B run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afd698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (System)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
