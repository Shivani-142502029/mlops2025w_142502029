{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674bf3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Define Model Architecture \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 8 * 8, 256), \n",
    "            nn.ReLU(),\n",
    "            # classifier.3 is the layer that changes size\n",
    "            nn.Linear(256, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 2. Define Training Utility Function (FIXED)\n",
    "def train_and_log(dataset_name, num_classes, init_model_path=None, project_name=\"Q4-Sequential-CIFAR\", epochs=10):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # W&B initialization for this specific run\n",
    "    run = wandb.init(\n",
    "        project=project_name, \n",
    "        group=f\"Sequence_{dataset_name}\",\n",
    "        name=f\"Train_{dataset_name}_init_{os.path.basename(init_model_path) if init_model_path else 'scratch'}\",\n",
    "        reinit=True,\n",
    "        config={\n",
    "            \"dataset\": dataset_name,\n",
    "            \"epochs\": epochs,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"batch_size\": 64,\n",
    "            \"model_architecture\": \"SimpleCNN\",\n",
    "            \"num_classes\": num_classes\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Initialize the model with the TARGET number of classes\n",
    "    model = SimpleCNN(num_classes=num_classes)\n",
    "\n",
    "    # Load initial weights if provided (FIXED TRANSFER LEARNING LOGIC)\n",
    "    if init_model_path and os.path.exists(init_model_path):\n",
    "        print(f\"Loading weights from: {init_model_path} for transfer...\")\n",
    "        # Use strict=False to ignore the size mismatch in the final classification layer.\n",
    "        # This successfully loads the feature weights but skips the head weights.\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(init_model_path, map_location=device), strict=False)\n",
    "            print(\"Successfully loaded feature weights (classification head weights ignored).\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during non-strict loading: {e}. Model will start from scratch.\")\n",
    "            \n",
    "    model.to(device)\n",
    "\n",
    "    # Data transformation and loading\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    if dataset_name == 'CIFAR-100':\n",
    "        train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "        test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "    else: # CIFAR-10\n",
    "        train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=0) # Changed num_workers to 0 for better compatibility\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        # ... Training loop and evaluation logic remain the same ...\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": total_loss / len(train_loader),\n",
    "            \"val_accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "        print(f\"[{dataset_name}] Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Acc: {accuracy:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), f'best_model_{dataset_name}.pt')\n",
    "\n",
    "    run.finish()\n",
    "    \n",
    "    # Save the final model state (not just the best) to be used for the next stage\n",
    "    final_path = f'final_model_{dataset_name}_{num_classes}.pt'\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd03e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing initial file system cleanup...\n",
      "Cleanup complete.\n",
      "\n",
      "--- Starting Sequence A: CIFAR-100 -> CIFAR-10 ---\n",
      "Stage A-1: Training CIFAR-100 from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\2021i\\Documents\\10_iit palakkad\\MLOPS\\Assignment 5\\wandb\\run-20251012_234737-oimbxgo4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/oimbxgo4' target=\"_blank\">Train_CIFAR-100_init_scratch</a></strong> to <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/oimbxgo4' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/oimbxgo4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-100] Epoch 1/10, Loss: 3.5727, Acc: 0.2559\n",
      "[CIFAR-100] Epoch 2/10, Loss: 2.8980, Acc: 0.3219\n",
      "[CIFAR-100] Epoch 3/10, Loss: 2.6033, Acc: 0.3612\n",
      "[CIFAR-100] Epoch 4/10, Loss: 2.4047, Acc: 0.3857\n",
      "[CIFAR-100] Epoch 5/10, Loss: 2.2578, Acc: 0.3954\n",
      "[CIFAR-100] Epoch 6/10, Loss: 2.1407, Acc: 0.4113\n",
      "[CIFAR-100] Epoch 7/10, Loss: 2.0444, Acc: 0.4268\n",
      "[CIFAR-100] Epoch 8/10, Loss: 1.9666, Acc: 0.4270\n",
      "[CIFAR-100] Epoch 9/10, Loss: 1.8956, Acc: 0.4355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-100] Epoch 10/10, Loss: 1.8295, Acc: 0.4404\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>1.82948</td></tr><tr><td>val_accuracy</td><td>0.4404</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train_CIFAR-100_init_scratch</strong> at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/oimbxgo4' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/oimbxgo4</a><br> View project at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251012_234737-oimbxgo4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved to model_cifar100_final.pt for transfer.\n",
      "\n",
      "Stage A-2: Fine-tuning CIFAR-10 using CIFAR-100 weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\2021i\\Documents\\10_iit palakkad\\MLOPS\\Assignment 5\\wandb\\run-20251013_000534-t8t250kg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/t8t250kg' target=\"_blank\">Train_CIFAR-10_init_model_cifar100_final.pt</a></strong> to <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/t8t250kg' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/t8t250kg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: model_cifar100_final.pt for transfer...\n",
      "Error during non-strict loading: Error(s) in loading state_dict for SimpleCNN:\n",
      "\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([10, 256]).\n",
      "\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([10]).. Model will start from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 170M/170M [06:55<00:00, 410kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-10] Epoch 1/10, Loss: 1.1388, Acc: 0.6812\n",
      "[CIFAR-10] Epoch 2/10, Loss: 0.8976, Acc: 0.7030\n",
      "[CIFAR-10] Epoch 3/10, Loss: 0.8121, Acc: 0.7204\n",
      "[CIFAR-10] Epoch 4/10, Loss: 0.7440, Acc: 0.7339\n",
      "[CIFAR-10] Epoch 5/10, Loss: 0.6897, Acc: 0.7448\n",
      "[CIFAR-10] Epoch 6/10, Loss: 0.6485, Acc: 0.7508\n",
      "[CIFAR-10] Epoch 7/10, Loss: 0.6070, Acc: 0.7556\n",
      "[CIFAR-10] Epoch 8/10, Loss: 0.5724, Acc: 0.7538\n",
      "[CIFAR-10] Epoch 9/10, Loss: 0.5419, Acc: 0.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-10] Epoch 10/10, Loss: 0.5150, Acc: 0.7529\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>0.51501</td></tr><tr><td>val_accuracy</td><td>0.7529</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train_CIFAR-10_init_model_cifar100_final.pt</strong> at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/t8t250kg' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/t8t250kg</a><br> View project at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251013_000534-t8t250kg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence A completed.\n",
      "\n",
      "--- Starting Sequence B: CIFAR-10 -> CIFAR-100 ---\n",
      "Stage B-1: Training CIFAR-10 from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\2021i\\Documents\\10_iit palakkad\\MLOPS\\Assignment 5\\wandb\\run-20251013_002654-5ma4077o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/5ma4077o' target=\"_blank\">Train_CIFAR-10_init_scratch</a></strong> to <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/5ma4077o' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/5ma4077o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-10] Epoch 1/10, Loss: 1.3723, Acc: 0.6037\n",
      "[CIFAR-10] Epoch 2/10, Loss: 1.0450, Acc: 0.6697\n",
      "[CIFAR-10] Epoch 3/10, Loss: 0.9029, Acc: 0.7069\n",
      "[CIFAR-10] Epoch 4/10, Loss: 0.8021, Acc: 0.7224\n",
      "[CIFAR-10] Epoch 5/10, Loss: 0.7204, Acc: 0.7335\n",
      "[CIFAR-10] Epoch 6/10, Loss: 0.6529, Acc: 0.7381\n",
      "[CIFAR-10] Epoch 7/10, Loss: 0.5923, Acc: 0.7545\n",
      "[CIFAR-10] Epoch 8/10, Loss: 0.5404, Acc: 0.7521\n",
      "[CIFAR-10] Epoch 9/10, Loss: 0.5019, Acc: 0.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-10] Epoch 10/10, Loss: 0.4602, Acc: 0.7510\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>0.46015</td></tr><tr><td>val_accuracy</td><td>0.751</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train_CIFAR-10_init_scratch</strong> at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/5ma4077o' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/5ma4077o</a><br> View project at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251013_002654-5ma4077o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved to model_cifar10_final.pt for transfer.\n",
      "\n",
      "Stage B-2: Fine-tuning CIFAR-100 using CIFAR-10 weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\2021i\\Documents\\10_iit palakkad\\MLOPS\\Assignment 5\\wandb\\run-20251013_003909-cro4gqd3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/cro4gqd3' target=\"_blank\">Train_CIFAR-100_init_model_cifar10_final.pt</a></strong> to <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/cro4gqd3' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/cro4gqd3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: model_cifar10_final.pt for transfer...\n",
      "Error during non-strict loading: Error(s) in loading state_dict for SimpleCNN:\n",
      "\tsize mismatch for classifier.3.weight: copying a param with shape torch.Size([10, 256]) from checkpoint, the shape in current model is torch.Size([100, 256]).\n",
      "\tsize mismatch for classifier.3.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([100]).. Model will start from scratch.\n",
      "[CIFAR-100] Epoch 1/10, Loss: 3.0358, Acc: 0.3642\n",
      "[CIFAR-100] Epoch 2/10, Loss: 2.3643, Acc: 0.4081\n",
      "[CIFAR-100] Epoch 3/10, Loss: 2.1099, Acc: 0.4263\n",
      "[CIFAR-100] Epoch 4/10, Loss: 1.9389, Acc: 0.4423\n",
      "[CIFAR-100] Epoch 5/10, Loss: 1.8015, Acc: 0.4535\n",
      "[CIFAR-100] Epoch 6/10, Loss: 1.6784, Acc: 0.4656\n",
      "[CIFAR-100] Epoch 7/10, Loss: 1.5746, Acc: 0.4629\n",
      "[CIFAR-100] Epoch 8/10, Loss: 1.4865, Acc: 0.4663\n",
      "[CIFAR-100] Epoch 9/10, Loss: 1.4061, Acc: 0.4682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIFAR-100] Epoch 10/10, Loss: 1.3431, Acc: 0.4671\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>1.34306</td></tr><tr><td>val_accuracy</td><td>0.4671</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Train_CIFAR-100_init_model_cifar10_final.pt</strong> at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/cro4gqd3' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR/runs/cro4gqd3</a><br> View project at: <a href='https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR' target=\"_blank\">https://wandb.ai/142502029-iit-palakkad/Q4-Sequential-CIFAR</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251013_003909-cro4gqd3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence B completed. All W&B runs finished.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "import glob\n",
    "import time # Use time.sleep to ensure file system operations complete, if needed\n",
    "\n",
    "# Configuration\n",
    "# Set NUM_EPOCHS to 100 for final submission, 10 is used for testing speed.\n",
    "NUM_EPOCHS = 10 \n",
    "CIFAR10_CLASSES = 10\n",
    "CIFAR100_CLASSES = 100\n",
    "\n",
    "# These are the required paths for the intermediate transfer weights\n",
    "MODEL_PATH_A = \"model_cifar100_final.pt\" # CIFAR-100 -> CIFAR-10 transfer file\n",
    "MODEL_PATH_B = \"model_cifar10_final.pt\"  # CIFAR-10 -> CIFAR-100 transfer file\n",
    "\n",
    "# Cleanup function to delete old files\n",
    "def cleanup_old_files():\n",
    "    # Delete the primary transfer files\n",
    "    for f in [MODEL_PATH_A, MODEL_PATH_B]:\n",
    "        if os.path.exists(f):\n",
    "            print(f\"Cleanup: Removing old transfer file {f}\")\n",
    "            os.remove(f)\n",
    "    # Also delete the temporary files created by train_and_log, just in case\n",
    "    for f in glob.glob(\"final_model_CIFAR*.pt\"):\n",
    "        if os.path.exists(f):\n",
    "            print(f\"Cleanup: Removing temporary file {f}\")\n",
    "            os.remove(f)\n",
    "    for f in glob.glob(\"best_model_CIFAR*.pt\"):\n",
    "        if os.path.exists(f):\n",
    "            print(f\"Cleanup: Removing temporary best file {f}\")\n",
    "            os.remove(f)\n",
    "    \n",
    "print(\"Performing initial file system cleanup...\")\n",
    "cleanup_old_files()\n",
    "print(\"Cleanup complete.\")\n",
    "time.sleep(1) # Wait a moment for file system to sync (especially on Windows)\n",
    "\n",
    "# 4a. Sequence A: Train CIFAR-100 (100 classes) -> Train CIFAR-10 (10 classes)\n",
    "\n",
    "print(\"Starting Sequence A: CIFAR-100 -> CIFAR-10\")\n",
    "\n",
    "# Stage A-1: Train CIFAR-100 from scratch \n",
    "print(\"Stage A-1: Training CIFAR-100 from scratch...\")\n",
    "# This call returns the path of the final saved model (e.g., final_model_CIFAR-100_100.pt)\n",
    "final_cifar100_path = train_and_log(\n",
    "    dataset_name='CIFAR-100',\n",
    "    num_classes=CIFAR100_CLASSES,\n",
    "    epochs=NUM_EPOCHS\n",
    ")\n",
    "# Move the newly trained model to the required transfer path (MODEL_PATH_A)\n",
    "os.rename(final_cifar100_path, MODEL_PATH_A)\n",
    "print(f\"Weights saved to {MODEL_PATH_A} for transfer.\")\n",
    "\n",
    "\n",
    "# Stage A-2: Fine-tune on CIFAR-10 using CIFAR-100 weights\n",
    "print(\"\\nStage A-2: Fine-tuning CIFAR-10 using CIFAR-100 weights...\")\n",
    "final_cifar10_path_A = train_and_log(\n",
    "    dataset_name='CIFAR-10',\n",
    "    num_classes=CIFAR10_CLASSES,\n",
    "    init_model_path=MODEL_PATH_A, # Use the model saved from Stage A-1\n",
    "    epochs=NUM_EPOCHS\n",
    ")\n",
    "# Cleanup the temporary final file name created by this stage\n",
    "if os.path.exists(final_cifar10_path_A):\n",
    "    os.remove(final_cifar10_path_A)\n",
    "print(\"Sequence A completed.\")\n",
    "\n",
    "\n",
    "# 4b. Sequence B: Train CIFAR-10 (10 classes) -> Train CIFAR-100 (100 classes)\n",
    "\n",
    "print(\"Starting Sequence B: CIFAR-10 -> CIFAR-100\")\n",
    "\n",
    "# Stage B-1: Train CIFAR-10 from scratch\n",
    "print(\"Stage B-1: Training CIFAR-10 from scratch...\")\n",
    "final_cifar10_path = train_and_log(\n",
    "    dataset_name='CIFAR-10',\n",
    "    num_classes=CIFAR10_CLASSES,\n",
    "    epochs=NUM_EPOCHS\n",
    ")\n",
    "# Move the newly trained model to the required transfer path (MODEL_PATH_B)\n",
    "os.rename(final_cifar10_path, MODEL_PATH_B)\n",
    "print(f\"Weights saved to {MODEL_PATH_B} for transfer.\")\n",
    "\n",
    "# Stage B-2: Fine-tune on CIFAR-100 using CIFAR-10 weights\n",
    "print(\"\\nStage B-2: Fine-tuning CIFAR-100 using CIFAR-10 weights...\")\n",
    "final_cifar100_path_B = train_and_log(\n",
    "    dataset_name='CIFAR-100',\n",
    "    num_classes=CIFAR100_CLASSES,\n",
    "    init_model_path=MODEL_PATH_B, # Use the model saved from Stage B-1\n",
    "    epochs=NUM_EPOCHS\n",
    ")\n",
    "# Cleanup the temporary final file name created by this stage\n",
    "if os.path.exists(final_cifar100_path_B):\n",
    "    os.remove(final_cifar100_path_B)\n",
    "print(\"Sequence B completed. All W&B runs finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e830e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (System)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
